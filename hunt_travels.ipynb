{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1U5McTQnZmji5kUunWwecB9uOTv38RdHH",
      "authorship_tag": "ABX9TyMyzJfU5d0sTY9uljvF0f2T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phenningsson/travels_of_hunt/blob/main/hunt_travels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "c-ULDLqhaL05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57640f4b-dec7-483b-e3d5-ce99661bb82c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.6)\n",
            "Requirement already satisfied: geopy in /usr/local/lib/python3.10/dist-packages (2.4.1)\n",
            "Requirement already satisfied: geographiclib<3,>=1.52 in /usr/local/lib/python3.10/dist-packages (from geopy) (2.0)\n",
            "Collecting en-core-web-lg==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# Install the required packages\n",
        "# spaCy for Namned Entity Recognition and pre-processing of the text\n",
        "# textblob for sentiment analysis\n",
        "# geopy for getting coordinates of extracted locations\n",
        "# Note: in order for the below code to work, you need to download the txt file\n",
        "# of Eleonora Hunt's travelogue \"My Trip Around the World\", available here\n",
        "# from Project Gutenberg: https://www.gutenberg.org/ebooks/33079\n",
        "# (also remove the Gutenberg disclaimers from the beginning and end of the text file\n",
        "# since we are interested in the text from the book itself)\n",
        "# Many thanks to Project Gutenberg for providing books for free publicly online\n",
        "!pip install spacy\n",
        "!pip install textblob\n",
        "!pip install geopy\n",
        "!python -m spacy download en_core_web_lg\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The below code in this cell extracts all the locations found\n",
        "# by spaCys NER. While interesting, it is far too many locations\n",
        "# and falls outside the scope of this project\n",
        "\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import re\n",
        "from geopy.geocoders import Nominatim\n",
        "import time\n",
        "\n",
        "# Load spaCy's large model (better Namned Entity Recognition accuracy)\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "\n",
        "# Initialize geolocator using Nominatim\n",
        "geolocator = Nominatim(user_agent=\"ph223ed@student.lnu.se\")\n",
        "\n",
        "# Get latitude and longitude coordinates of location, throw error message if not found\n",
        "def get_coordinates(location_name):\n",
        "    try:\n",
        "        # Add delay to the Nominatim API calls\n",
        "        time.sleep(1)\n",
        "        location = geolocator.geocode(location_name)\n",
        "        if location:\n",
        "            return (location.latitude, location.longitude)\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting coordinates for {location_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Normalize name of location (some are spelled in all caps in the text)\n",
        "def normalize_location_name(location):\n",
        "    # Convert to string and remove whitespaces in the beginning and end\n",
        "    location = str(location).strip()\n",
        "\n",
        "    # Convert to title case\n",
        "    location = location.title()\n",
        "\n",
        "    # Remove 'The' from beginning if present\n",
        "    if location.startswith('The '):\n",
        "        location = location[4:]\n",
        "\n",
        "    # Remove periods and redundant whitespaces\n",
        "    location = re.sub(r'\\.', '', location)\n",
        "    location = re.sub(r'\\s+', ' ', location)\n",
        "\n",
        "    return location.strip()\n",
        "\n",
        "# Extract locations with context (text that mentions/describes the location)\n",
        "def extract_locations_with_context(text):\n",
        "    doc = nlp(text)\n",
        "    locations = []\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in ['GPE', 'LOC']:  # GPE for cities/countries, LOC for locations\n",
        "            # Get the sentence containing the location for context\n",
        "            sent = [sent for sent in doc.sents if ent.start >= sent.start and ent.end <= sent.end][0]\n",
        "\n",
        "            # Normalize location name\n",
        "            location_name = normalize_location_name(ent.text)\n",
        "\n",
        "            # Get coordinates\n",
        "            coordinates = get_coordinates(location_name)\n",
        "\n",
        "            locations.append({\n",
        "                'original_name': ent.text,\n",
        "                'normalized_name': location_name,\n",
        "                'entity_type': ent.label_,\n",
        "                'context': sent.text,\n",
        "                'coordinates': coordinates\n",
        "            })\n",
        "\n",
        "    return locations\n",
        "\n",
        "# Read text file\n",
        "with open('hunt_travel.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Get the locations and store in a data frame\n",
        "locations = extract_locations_with_context(text)\n",
        "df = pd.DataFrame(locations)\n",
        "\n",
        "# Split coordinates column into separate latitude and longitude columns\n",
        "df['latitude'] = df['coordinates'].apply(lambda x: x[0] if x else None)\n",
        "df['longitude'] = df['coordinates'].apply(lambda x: x[1] if x else None)\n",
        "\n",
        "# Drop the coordinates column\n",
        "df = df.drop('coordinates', axis=1)\n",
        "\n",
        "# Sort by normalized name (but keep duplicates since we want all mentions of a given location)\n",
        "df = df.sort_values('normalized_name')\n",
        "\n",
        "# Print the total of all location mentions\n",
        "# Print sample of locations along with their extracted coordinates\n",
        "print(f\"Found {len(df)} location mentions\")\n",
        "print(\"\\nSample of locations with coordinates:\")\n",
        "print(df[['normalized_name', 'latitude', 'longitude']].head())\n",
        "\n",
        "# Save name of location and its coordinates to a CSV file\n",
        "df.to_csv('extracted_locations_with_coordinates.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiF0880n33DU",
        "outputId": "2e2b6098-3c8c-4040-9f32-067465682895"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\")': /search?q=Saigon&format=json&limit=1\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\")': /search?q=St+Lawrence+River&format=json&limit=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 502 location mentions\n",
            "\n",
            "Sample of locations with coordinates:\n",
            "    normalized_name   latitude  longitude\n",
            "325           Abdin  13.566941  29.637362\n",
            "335       Abyssinia  10.211670  38.652120\n",
            "248            Aden  12.789585  45.028504\n",
            "247            Aden  12.789585  45.028504\n",
            "253            Aden  12.789585  45.028504\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The below code in this cell is used specifically for this project, and\n",
        "# does the same as the code above but only looks at the main stops from the text\n",
        "\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import re\n",
        "from geopy.geocoders import Nominatim\n",
        "import time\n",
        "\n",
        "# Load spaCy's large model (better Namned Entity Recognition accuracy)\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "\n",
        "# Define the main stops in chronological order with the same spelling as they occur in the text\n",
        "main_stops = [\n",
        "    'Chicago',\n",
        "    'Glaciers',\n",
        "    'Vancouver',\n",
        "    'Yokohama',\n",
        "    'Niko',\n",
        "    'Tokio',\n",
        "    'Kobe',\n",
        "    'Nagasaki',\n",
        "    'Shanghai',\n",
        "    'Hongkong',\n",
        "    'Canton',\n",
        "    'Saigon',\n",
        "    'Singapore',\n",
        "    'Kandy',\n",
        "    'Calcutta',\n",
        "    'Benares',\n",
        "    'Aigra',\n",
        "    'Delhi',\n",
        "    'Jeypore',\n",
        "    'Bombay',\n",
        "    'Aden',\n",
        "    'Ismalia',\n",
        "    'Cairo',\n",
        "    'Luxor',\n",
        "    'Brindisi',\n",
        "    'Marseilles',\n",
        "    'Paris',\n",
        "    'London',\n",
        "    'New York'\n",
        "]\n",
        "\n",
        "# Initialize geolocator using Nominatim\n",
        "geolocator = Nominatim(user_agent=\"ph223ed@student.lnu.se\")\n",
        "\n",
        "# Get latitude and longitude coordinates of location, throws error message if not found\n",
        "def get_coordinates(location_name):\n",
        "    try:\n",
        "          # Add delay to the Nominatim API calls to respect API limits\n",
        "        time.sleep(1)\n",
        "        location = geolocator.geocode(location_name)\n",
        "        if location:\n",
        "            return (location.latitude, location.longitude)\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting coordinates for {location_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_locations_with_context(text):\n",
        "    doc = nlp(text)\n",
        "    locations = []\n",
        "\n",
        "    # Process each sentence\n",
        "    for sent in doc.sents:\n",
        "        # Check if any main stop is mentioned in the sentence\n",
        "        for location in main_stops:\n",
        "            if location.lower() in sent.text.lower():\n",
        "                # Get surrounding sentences for context\n",
        "                sent_index = list(doc.sents).index(sent)\n",
        "                start_index = max(0, sent_index - 1)\n",
        "                end_index = min(len(list(doc.sents)), sent_index + 2)\n",
        "\n",
        "                # Get context (2 sentences, one before and one after location mention)\n",
        "                context_sents = list(doc.sents)[start_index:end_index]\n",
        "                context = ' '.join([s.text.strip() for s in context_sents])\n",
        "\n",
        "                # Get coordinates\n",
        "                coordinates = get_coordinates(location)\n",
        "\n",
        "                locations.append({\n",
        "                    'original_name': location,\n",
        "                    'normalized_name': location,  # We're using original spellings\n",
        "                    'context': context,\n",
        "                    'coordinates': coordinates\n",
        "                })\n",
        "\n",
        "    return locations\n",
        "\n",
        "# Read the text file\n",
        "with open('hunt_travel.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Get locations and store in a data frame\n",
        "locations = extract_locations_with_context(text)\n",
        "df = pd.DataFrame(locations)\n",
        "\n",
        "# Split coordinates into separate latitude and longitude columns\n",
        "df['latitude'] = df['coordinates'].apply(lambda x: x[0] if x else None)\n",
        "df['longitude'] = df['coordinates'].apply(lambda x: x[1] if x else None)\n",
        "\n",
        "# Drop the coordinates column\n",
        "df = df.drop('coordinates', axis=1)\n",
        "\n",
        "# Sort by the order in main_stops (to maintain chronological order)\n",
        "df['sort_order'] = df['original_name'].map({name: i for i, name in enumerate(main_stops)})\n",
        "df = df.sort_values('sort_order')\n",
        "df = df.drop('sort_order', axis=1)\n",
        "\n",
        "# Print the total number of location mentions\n",
        "# Print sample of locations along with their extracted coordinates\n",
        "print(f\"Found {len(df)} location mentions\")\n",
        "print(\"\\nSample of locations with coordinates:\")\n",
        "print(df[['original_name', 'normalized_name', 'latitude', 'longitude']].head())\n",
        "\n",
        "# # Save name of location and its coordinates to a CSV file\n",
        "df.to_csv('ext_loc_cord.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uud6DFbTRzks",
        "outputId": "2e7466dd-b538-4d75-a3e7-6aeef41b53e3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\")': /search?q=Kobe&format=json&limit=1\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='nominatim.openstreetmap.org', port=443): Read timed out. (read timeout=1)\")': /search?q=Kobe&format=json&limit=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 195 location mentions\n",
            "\n",
            "Sample of locations with coordinates:\n",
            "    original_name normalized_name   latitude  longitude\n",
            "0         Chicago         Chicago  41.875562 -87.624421\n",
            "79        Chicago         Chicago  41.875562 -87.624421\n",
            "80        Chicago         Chicago  41.875562 -87.624421\n",
            "127       Chicago         Chicago  41.875562 -87.624421\n",
            "169       Chicago         Chicago  41.875562 -87.624421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Read the CSV file created in the above cell containing locations and their coordinates\n",
        "df = pd.read_csv('/content/ext_loc_cord.csv')\n",
        "\n",
        "# Calculate sentiment scores\n",
        "def get_sentiment(text):\n",
        "    blob = TextBlob(str(text))\n",
        "    return blob.sentiment.polarity\n",
        "\n",
        "# Add sentiment scores to data frame\n",
        "df['sentiment_score'] = df['context'].apply(get_sentiment)\n",
        "\n",
        "# Save all entries with their individual sentiment scores to new CSV file\n",
        "detailed_sentiments = df[['original_name', 'normalized_name', 'context', 'sentiment_score', 'latitude', 'longitude']]\n",
        "detailed_sentiments.to_csv('ind_loc_sentiments.csv', index=False)\n",
        "\n",
        "# Calculate average sentiment for locations with multiple mentions,\n",
        "# also calculates standard deviation of sentiment score and the location mention count\n",
        "location_sentiments = df.groupby('normalized_name').agg({\n",
        "    'sentiment_score': ['mean', 'count', 'std'],\n",
        "    'latitude': 'first',\n",
        "    'longitude': 'first'\n",
        "}).round(3)\n",
        "\n",
        "# Clean up the column names\n",
        "location_sentiments.columns = ['avg_sentiment', 'mention_count', 'sentiment_std', 'latitude', 'longitude']\n",
        "location_sentiments = location_sentiments.reset_index()\n",
        "\n",
        "# Sort by average sentiment\n",
        "location_sentiments = location_sentiments.sort_values('avg_sentiment', ascending=False)\n",
        "\n",
        "# Save average sentiments to new CSV file\n",
        "location_sentiments.to_csv('avg_loc_sentiments.csv', index=False)\n",
        "\n",
        "# Calculate overall sentiment statistics; average sentiment, most positive and negative\n",
        "overall_avg_sentiment = df['sentiment_score'].mean()\n",
        "most_positive = location_sentiments.iloc[0]\n",
        "most_negative = location_sentiments.iloc[-1]\n",
        "\n",
        "# Print sentiment statistics\n",
        "print(\"\\nSentiment Analysis Summary:\")\n",
        "print(f\"Average sentiment across all mentions: {overall_avg_sentiment:.3f}\")\n",
        "print(f\"Most positive location: {most_positive['normalized_name']} ({most_positive['avg_sentiment']:.3f})\")\n",
        "print(f\"Most negative location: {most_negative['normalized_name']} ({most_negative['avg_sentiment']:.3f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqnYmz9hV4NN",
        "outputId": "4cf58a33-8608-48b9-8f6c-bcb98522a975"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentiment Analysis Summary:\n",
            "Average sentiment across all mentions: 0.155\n",
            "Most positive location: Yokohama (0.334)\n",
            "Most negative location: Brindisi (-0.123)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a dictionary mapping locations to their chronological order\n",
        "chronological_order = {\n",
        "    'Chicago': 1,\n",
        "    'Glaciers': 2,\n",
        "    'Vancouver': 3,\n",
        "    'Yokohama': 4,\n",
        "    'Niko': 5,\n",
        "    'Tokio': 6,\n",
        "    'Kobe': 7,\n",
        "    'Nagasaki': 8,\n",
        "    'Shanghai': 9,\n",
        "    'Hongkong': 10,\n",
        "    'Canton': 11,\n",
        "    'Saigon': 12,\n",
        "    'Singapore': 13,\n",
        "    'Kandy': 14,\n",
        "    'Calcutta': 15,\n",
        "    'Benares': 16,\n",
        "    'Aigra': 17,\n",
        "    'Delhi': 18,\n",
        "    'Jeypore': 19,\n",
        "    'Bombay': 20,\n",
        "    'Aden': 21,\n",
        "    'Ismalia': 22,\n",
        "    'Cairo': 23,\n",
        "    'Luxor': 24,\n",
        "    'Brindisi': 25,\n",
        "    'Marseilles': 26,\n",
        "    'Paris': 27,\n",
        "    'London': 28,\n",
        "    'New York': 29\n",
        "}\n",
        "\n",
        "# Read the CSV file created in the above cell\n",
        "df = pd.read_csv('avg_loc_sentiments.csv')\n",
        "\n",
        "# Add chronological order column, used for mapping the order of stops in QGIS\n",
        "df['chronological_order'] = df['normalized_name'].map(chronological_order)\n",
        "\n",
        "# Sort by chronological order\n",
        "df = df.sort_values('chronological_order')\n",
        "\n",
        "# Save to new CSV file\n",
        "df.to_csv('avg_loc_sentiments_ordered.csv', index=False)\n",
        "\n",
        "# Print first few rows to verify\n",
        "print(df[['normalized_name', 'chronological_order', 'avg_sentiment']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHNLcktjXV_D",
        "outputId": "c3a3b8ee-ffe8-419e-db5f-0e4d2cbee150"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   normalized_name  chronological_order  avg_sentiment\n",
            "5          Chicago                    1          0.212\n",
            "27        Glaciers                    2         -0.012\n",
            "18       Vancouver                    3          0.126\n",
            "0         Yokohama                    4          0.334\n",
            "21            Niko                    5          0.094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a dictionary with arrival and departure times\n",
        "# Based on explicit or approximate arrival and departure times in the text\n",
        "# For simplicity's sake, only takes into account first arrival and departure time\n",
        "# (For instance, Cairo is being visisted twice, the second arrival and departure date is disregarded in the below code)\n",
        "time_data = {\n",
        "    'Chicago': {'arrival': '1895-08-18T00:00:00Z', 'departure': '1895-08-19T00:00:00Z'},\n",
        "    'Glaciers': {'arrival': '1895-08-22T00:00:00Z', 'departure': '1895-08-23T00:00:00Z'},\n",
        "    'Vancouver': {'arrival': '1895-08-24T00:00:00Z', 'departure': '1895-08-26T00:00:00Z'},\n",
        "    'Yokohama': {'arrival': '1895-09-09T00:00:00Z', 'departure': '1895-09-10T00:00:00Z'},\n",
        "    'Niko': {'arrival': '1895-09-11T00:00:00Z', 'departure': '1895-09-12T00:00:00Z'},\n",
        "    'Tokio': {'arrival': '1895-09-13T00:00:00Z', 'departure': '1895-09-14T00:00:00Z'},\n",
        "    'Kobe': {'arrival': '1895-09-17T00:00:00Z', 'departure': '1895-09-18T00:00:00Z'},\n",
        "    'Nagasaki': {'arrival': '1895-10-01T00:00:00Z', 'departure': '1895-10-02T00:00:00Z'},\n",
        "    'Shanghai': {'arrival': '1895-10-04T00:00:00Z', 'departure': '1895-10-07T00:00:00Z'},\n",
        "    'Hongkong': {'arrival': '1895-10-08T00:00:00Z', 'departure': '1895-10-09T00:00:00Z'},\n",
        "    'Canton': {'arrival': '1895-10-10T00:00:00Z', 'departure': '1895-10-11T00:00:00Z'},\n",
        "    'Saigon': {'arrival': '1895-10-19T00:00:00Z', 'departure': '1895-10-20T00:00:00Z'},\n",
        "    'Singapore': {'arrival': '1895-10-22T00:00:00Z', 'departure': '1895-10-23T00:00:00Z'},\n",
        "    'Kandy': {'arrival': '1895-10-30T00:00:00Z', 'departure': '1895-11-01T00:00:00Z'},\n",
        "    'Calcutta': {'arrival': '1895-11-05T00:00:00Z', 'departure': '1895-11-17T00:00:00Z'},\n",
        "    'Benares': {'arrival': '1895-11-18T00:00:00Z', 'departure': '1895-11-19T00:00:00Z'},\n",
        "    'Aigra': {'arrival': '1895-11-20T00:00:00Z', 'departure': '1895-11-21T00:00:00Z'},\n",
        "    'Delhi': {'arrival': '1895-11-22T00:00:00Z', 'departure': '1895-11-23T00:00:00Z'},\n",
        "    'Jeypore': {'arrival': '1895-11-28T00:00:00Z', 'departure': '1895-12-02T00:00:00Z'},\n",
        "    'Bombay': {'arrival': '1895-12-04T00:00:00Z', 'departure': '1895-12-16T00:00:00Z'},\n",
        "    'Aden': {'arrival': '1895-12-17T11:00:00Z', 'departure': '1895-12-17T23:00:00Z'},\n",
        "    'Ismalia': {'arrival': '1895-12-21T12:30:00Z', 'departure': '1895-12-21T18:00:00Z'},\n",
        "    'Cairo': {'arrival': '1895-12-22T18:00:00Z', 'departure': '1895-12-24T00:00:00Z'},\n",
        "    'Luxor': {'arrival': '1896-01-01T00:00:00Z', 'departure': '1896-01-18T00:00:00Z'},\n",
        "    'Brindisi': {'arrival': '1896-02-01T00:00:00Z', 'departure': '1896-02-02T00:00:00Z'},\n",
        "    'Marseilles': {'arrival': '1896-02-07T00:00:00Z', 'departure': '1896-02-08T00:00:00Z'},\n",
        "    'Paris': {'arrival': '1896-02-09T00:00:00Z', 'departure': '1896-02-15T00:00:00Z'},\n",
        "    'London': {'arrival': '1896-02-16T00:00:00Z', 'departure': '1896-02-28T00:00:00Z'},\n",
        "    'New York': {'arrival': '1896-03-10T00:00:00Z', 'departure': '1896-03-11T00:00:00Z'}\n",
        "}\n",
        "\n",
        "# Read CSV file created in the above cell (does not contain arrival departure time)\n",
        "df = pd.read_csv('avg_loc_sentiments_ordered.csv')\n",
        "\n",
        "# Add arrival and departure time columns to the dataset\n",
        "df['arrival_time'] = df['normalized_name'].map(lambda x: time_data[x]['arrival'] if x in time_data else None)\n",
        "df['departure_time'] = df['normalized_name'].map(lambda x: time_data[x]['departure'] if x in time_data else None)\n",
        "\n",
        "# Save updated dataset to new CSV file\n",
        "df.to_csv('loc_with_times.csv', index=False)"
      ],
      "metadata": {
        "id": "XC6fJn56hEj6"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}